# Sales Transactions API - ƒê∆°n Gi·∫£n H√≥a

**Format:** Ch·ªâ g·ª≠i d·ªØ li·ªáu giao d·ªãch b√°n h√†ng v√†o Kafka  
**Nguy√™n t·∫Øc:** 1 s·∫£n ph·∫©m = 1 event ri√™ng  
**Fields:** Ch·ªâ 6 tr∆∞·ªùng c·∫ßn thi·∫øt

---

## üìä Event Format

### Transaction Event Structure

```json
{
  "order_id": 123,
  "product_id": 456,
  "quantity": 2,
  "price": 500000.00,
  "timestamp": "2024-11-22T10:30:00+07:00",
  "customer_id": 789
}
```

**Kh√¥ng c√≥ field d∆∞ th·ª´a!** Ch·ªâ 6 fields n√†y.

---

## üöÄ API Endpoint

### POST `/api/kafka/transactions/sales`

G·ª≠i t·∫•t c·∫£ s·∫£n ph·∫©m trong 1 order v√†o Kafka (m·ªói product = 1 event ri√™ng).

#### Request

**URL:**
```
POST http://localhost:8000/api/kafka/transactions/sales
```

**Headers:**
```
Authorization: Bearer YOUR_ACCESS_TOKEN
Content-Type: application/json
```

**Body:**
```json
{
  "order_id": 123
}
```

#### Response (Success)

```json
{
  "status": "success",
  "message": "Sales transactions sent to Kafka successfully",
  "order_id": 123,
  "results": {
    "success": 3,
    "failed": 0,
    "total": 3
  },
  "format": {
    "order_id": "integer",
    "product_id": "integer",
    "quantity": "integer",
    "price": "decimal",
    "timestamp": "ISO8601 string",
    "customer_id": "integer"
  }
}
```

**Gi·∫£i th√≠ch:** Order c√≥ 3 s·∫£n ph·∫©m ‚Üí g·ª≠i 3 events ri√™ng bi·ªát v√†o Kafka.

#### Response (Error)

```json
{
  "status": "error",
  "message": "Order not found"
}
```

---

## üí° V√≠ D·ª• Th·ª±c T·∫ø

### Scenario: Order c√≥ 3 s·∫£n ph·∫©m

**Order #123 bao g·ªìm:**
- Product #1: K√≠nh m√°t Ray-Ban (qty: 1, price: 1,500,000 VND)
- Product #2: G·ªçng k√≠nh Gucci (qty: 2, price: 2,000,000 VND)
- Product #3: Tr√≤ng k√≠nh ch·ªëng √°nh s√°ng xanh (qty: 1, price: 500,000 VND)

**Customer:** User ID 789

---

### Request

```bash
curl -X POST http://localhost:8000/api/kafka/transactions/sales \
  -H "Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGc..." \
  -H "Content-Type: application/json" \
  -d '{
    "order_id": 123
  }'
```

---

### Kafka s·∫Ω nh·∫≠n 3 events ri√™ng bi·ªát:

#### Event 1: K√≠nh m√°t Ray-Ban
```json
{
  "order_id": 123,
  "product_id": 1,
  "quantity": 1,
  "price": 1500000.00,
  "timestamp": "2024-11-22T10:30:00+07:00",
  "customer_id": 789
}
```

#### Event 2: G·ªçng k√≠nh Gucci
```json
{
  "order_id": 123,
  "product_id": 2,
  "quantity": 2,
  "price": 2000000.00,
  "timestamp": "2024-11-22T10:30:00+07:00",
  "customer_id": 789
}
```

#### Event 3: Tr√≤ng k√≠nh
```json
{
  "order_id": 123,
  "product_id": 3,
  "quantity": 1,
  "price": 500000.00,
  "timestamp": "2024-11-22T10:30:00+07:00",
  "customer_id": 789
}
```

---

## üîß Kafka Topic

**Topic Name:** `order-created` (ho·∫∑c c√≥ th·ªÉ ƒë·ªïi th√†nh `sales-transactions`)

**Partitions:** 3  
**Replication Factor:** 1  
**Retention:** 7 days

---

## üìä Database Schema (Simplified)

### Table: sales_transactions

N·∫øu mu·ªën store transactions t·ª´ Kafka v√†o PostgreSQL (via Flink):

```sql
CREATE TABLE sales_transactions (
    id SERIAL PRIMARY KEY,
    order_id INTEGER NOT NULL,
    product_id INTEGER NOT NULL,
    quantity INTEGER NOT NULL,
    price DECIMAL(15, 2) NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    customer_id INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- Indexes
    INDEX idx_order_id (order_id),
    INDEX idx_product_id (product_id),
    INDEX idx_customer_id (customer_id),
    INDEX idx_timestamp (timestamp)
);
```

---

## üéØ Flink Job (Simplified)

### Job: SalesTransactionProcessor

**Input:** Kafka topic `order-created` ho·∫∑c `sales-transactions`

**Processing:**
```
1. Deserialize JSON ‚Üí SalesTransaction object
2. Validate fields (kh√¥ng null, price > 0, quantity > 0)
3. Write to PostgreSQL: sales_transactions table
```

**Code skeleton (Java):**

```java
public class SalesTransactionProcessor {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        
        // Kafka Source
        KafkaSource<String> kafkaSource = KafkaSource.<String>builder()
            .setBootstrapServers("kafka:29092")
            .setTopics("order-created")
            .setGroupId("sales-transaction-processor")
            .setValueOnlyDeserializer(new SimpleStringSchema())
            .build();
        
        // Process stream
        DataStream<SalesTransaction> transactions = env
            .fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "Kafka")
            .map(json -> parseSalesTransaction(json))
            .filter(t -> t != null && isValid(t));
        
        // Sink to PostgreSQL
        transactions.addSink(new JdbcSink<>(
            "INSERT INTO sales_transactions " +
            "(order_id, product_id, quantity, price, timestamp, customer_id) " +
            "VALUES (?, ?, ?, ?, ?, ?)",
            (ps, t) -> {
                ps.setInt(1, t.orderId);
                ps.setInt(2, t.productId);
                ps.setInt(3, t.quantity);
                ps.setBigDecimal(4, t.price);
                ps.setTimestamp(5, t.timestamp);
                ps.setInt(6, t.customerId);
            },
            jdbcConnectionOptions
        ));
        
        env.execute("Sales Transaction Processor");
    }
    
    private static SalesTransaction parseSalesTransaction(String json) {
        // Parse JSON to object
    }
    
    private static boolean isValid(SalesTransaction t) {
        return t.quantity > 0 && t.price.compareTo(BigDecimal.ZERO) > 0;
    }
}
```

---

## üß™ Testing

### 1. Test v·ªõi cURL

```bash
# Test endpoint
curl -X POST http://localhost:8000/api/kafka/transactions/sales \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"order_id": 1}'
```

### 2. Verify trong Kafka UI

1. M·ªü http://localhost:8080
2. Click **Topics** ‚Üí **order-created**
3. Click **Messages** tab
4. Xem events v·ª´a g·ª≠i

### 3. Consume t·ª´ Kafka (command line)

```bash
docker exec kafka /usr/bin/kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic order-created \
  --from-beginning \
  --max-messages 10
```

### 4. Verify trong PostgreSQL (sau khi c√≥ Flink job)

```bash
docker exec -it postgres psql -U postgres -d lensart_events

# Query transactions
SELECT * FROM sales_transactions 
WHERE order_id = 123 
ORDER BY product_id;
```

---

## üìù Use Cases

### 1. Real-time Analytics
- T√≠nh t·ªïng doanh thu theo s·∫£n ph·∫©m
- Top 10 s·∫£n ph·∫©m b√°n ch·∫°y
- Doanh thu theo gi·ªù/ng√†y/th√°ng

### 2. Inventory Management
- Track s·ªë l∆∞·ª£ng b√°n ra
- Alert khi stock th·∫•p
- Forecast demand

### 3. Customer Analytics
- S·∫£n ph·∫©m ph·ªï bi·∫øn theo customer segment
- Purchase patterns
- Recommendation engine data

### 4. Business Intelligence
- Dashboard real-time
- Sales reports
- Trend analysis

---

## ‚öôÔ∏è Configuration

### Kafka Producer Config (`config/kafka.php`)

```php
'topics' => [
    'order_created' => env('KAFKA_ORDER_CREATED_TOPIC', 'order-created'),
    // ho·∫∑c ri√™ng:
    'sales_transactions' => env('KAFKA_SALES_TRANSACTIONS_TOPIC', 'sales-transactions'),
],
```

### Environment Variables (`.env`)

```env
KAFKA_BROKERS=localhost:9092
KAFKA_ORDER_CREATED_TOPIC=order-created
# ho·∫∑c ri√™ng:
KAFKA_SALES_TRANSACTIONS_TOPIC=sales-transactions
```

---

## üîç Troubleshooting

### Issue 1: Order kh√¥ng c√≥ s·∫£n ph·∫©m

```json
{
  "status": "success",
  "results": {
    "success": 0,
    "failed": 0,
    "total": 0
  }
}
```

**Gi·∫£i ph√°p:** Check order_details table c√≥ data kh√¥ng.

---

### Issue 2: M·ªôt s·ªë events failed

```json
{
  "status": "error",
  "results": {
    "success": 2,
    "failed": 1,
    "total": 3
  }
}
```

**Gi·∫£i ph√°p:** 
- Check Kafka logs: `docker logs kafka --tail 50`
- Check Laravel logs: `storage/logs/laravel.log`
- Verify Kafka connection

---

### Issue 3: Events kh√¥ng xu·∫•t hi·ªán trong Kafka

**Check:**
1. Kafka service running: `docker ps | grep kafka`
2. Topic exists: `docker exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092`
3. Laravel logs: `tail -f storage/logs/laravel.log`

---

## üöÄ Deployment Checklist

### Development
- [x] API endpoint implemented
- [x] KafkaService method created
- [x] Route registered
- [ ] Test v·ªõi Postman/cURL
- [ ] Verify events in Kafka UI

### Production
- [ ] Authentication enabled
- [ ] Rate limiting configured
- [ ] Error monitoring (Sentry, etc.)
- [ ] Kafka cluster setup
- [ ] Flink job deployed
- [ ] Database replicated
- [ ] Backup strategy

---

## üìä Performance Considerations

### Throughput
- **Async sending:** KafkaService g·ª≠i async
- **Batch processing:** Flink x·ª≠ l√Ω batch
- **Expected load:** 1000 transactions/minute

### Scaling
- **Kafka partitions:** 3 (c√≥ th·ªÉ tƒÉng)
- **Flink parallelism:** 4 task slots
- **PostgreSQL:** Connection pooling

### Monitoring
- Kafka lag monitoring
- Flink checkpoint success rate
- Database write throughput

---

## üìö References

- [Kafka Documentation](https://kafka.apache.org/documentation/)
- [Flink Kafka Connector](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/kafka/)
- [Laravel Kafka Integration](../KAFKA_SETUP.md)

---

## ‚ú® Summary

**What changed:**
- ‚úÖ Simplified event format (ch·ªâ 6 fields)
- ‚úÖ 1 product = 1 event (kh√¥ng group)
- ‚úÖ Clean data structure (no nested objects)
- ‚úÖ Easy to process in Flink

**Benefits:**
- üöÄ Simpler to process
- üìä Easy to aggregate
- üîç Better for analytics
- ‚ö° Better performance

---

**Version:** 1.0.0  
**Last Updated:** 22/11/2024  
**Status:** ‚úÖ Ready to use

