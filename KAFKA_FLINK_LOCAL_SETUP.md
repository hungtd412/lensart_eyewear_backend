# Local Development Setup - Kafka + Flink + PostgreSQL vá»›i Docker

## ğŸ“‹ Tá»•ng quan Kiáº¿n trÃºc

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Laravel API    â”‚
â”‚  (Port 8000)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP POST (Order Events)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Broker   â”‚
â”‚  (Port 9092)    â”‚
â”‚  Topics:        â”‚
â”‚  - order-events â”‚
â”‚  - order-createdâ”‚
â”‚  - order-updatedâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Stream Events
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Flink   â”‚
â”‚  (Port 8081)    â”‚
â”‚  - Job Manager  â”‚
â”‚  - Task Manager â”‚
â”‚  Process & ETL  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Write Processed Data
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL     â”‚
â”‚  (Port 5432)    â”‚
â”‚  Database:      â”‚
â”‚  - order_events â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Má»¥c tiÃªu

1. **Laravel API** gá»­i order events qua HTTP API
2. **Kafka** nháº­n vÃ  lÆ°u trá»¯ events trong topics
3. **Flink** Ä‘á»c events tá»« Kafka, xá»­ lÃ½/transform data
4. **PostgreSQL** lÆ°u trá»¯ data Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½

## ğŸ³ Docker Services

### 1. Apache Kafka + Zookeeper
- **Zookeeper**: Quáº£n lÃ½ cluster Kafka (port 2181)
- **Kafka Broker**: Message broker (port 9092)
- **Kafka UI**: Web interface Ä‘á»ƒ monitor (port 8080)

### 2. Apache Flink
- **Job Manager**: Äiá»u phá»‘i jobs (port 8081 - Web UI)
- **Task Manager**: Thá»±c thi tasks

### 3. PostgreSQL
- **Database**: LÆ°u trá»¯ processed events (port 5432)
- **PgAdmin**: Web UI quáº£n lÃ½ database (port 5050)

## ğŸ“Š Data Flow Chi tiáº¿t

### Flow 1: Order Created
```
1. User táº¡o Ä‘Æ¡n hÃ ng â†’ Laravel API
2. Laravel gá»­i event â†’ POST /api/kafka/events/order-created
3. KafkaService push event â†’ Kafka topic "order-created"
4. Flink Job consume event tá»« Kafka
5. Flink transform data:
   - Validate order data
   - Enrich vá»›i thÃ´ng tin bá»• sung
   - Calculate metrics (total orders, revenue)
6. Flink sink data â†’ PostgreSQL tables:
   - orders_raw: Raw event data
   - orders_processed: Processed order info
   - order_metrics: Aggregated metrics
```

### Flow 2: Order Status Changed
```
1. Admin thay Ä‘á»•i tráº¡ng thÃ¡i Ä‘Æ¡n hÃ ng
2. Laravel gá»­i event â†’ POST /api/kafka/events/order-status-changed
3. Event â†’ Kafka topic "order-events"
4. Flink Job consume vÃ  process:
   - Track status history
   - Calculate processing time
   - Update metrics
5. Write to PostgreSQL:
   - order_status_history
   - order_metrics (update)
```

### Flow 3: Real-time Analytics
```
1. Flink continuously processes events
2. Calculate real-time metrics:
   - Orders per minute/hour
   - Revenue by branch
   - Top products
   - Customer behavior
3. Store in PostgreSQL analytics tables
4. (Optional) Expose via API for dashboard
```

## ğŸ—„ï¸ Database Schema (PostgreSQL)

### Table: orders_raw
```sql
CREATE TABLE orders_raw (
    id SERIAL PRIMARY KEY,
    event_id VARCHAR(100) UNIQUE NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    order_id INTEGER NOT NULL,
    user_id INTEGER,
    branch_id INTEGER,
    total_price DECIMAL(15, 2),
    order_status VARCHAR(50),
    payment_status VARCHAR(50),
    payment_method VARCHAR(50),
    event_data JSONB,
    event_timestamp TIMESTAMP,
    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Table: orders_processed
```sql
CREATE TABLE orders_processed (
    id SERIAL PRIMARY KEY,
    order_id INTEGER UNIQUE NOT NULL,
    user_id INTEGER,
    user_name VARCHAR(255),
    user_email VARCHAR(255),
    branch_id INTEGER,
    branch_name VARCHAR(255),
    order_date TIMESTAMP,
    total_price DECIMAL(15, 2),
    order_status VARCHAR(50),
    payment_status VARCHAR(50),
    payment_method VARCHAR(50),
    items_count INTEGER,
    created_at TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Table: order_status_history
```sql
CREATE TABLE order_status_history (
    id SERIAL PRIMARY KEY,
    order_id INTEGER NOT NULL,
    old_status VARCHAR(50),
    new_status VARCHAR(50),
    changed_at TIMESTAMP,
    processing_time_seconds INTEGER,
    event_id VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Table: order_metrics
```sql
CREATE TABLE order_metrics (
    id SERIAL PRIMARY KEY,
    metric_date DATE NOT NULL,
    metric_hour INTEGER,
    branch_id INTEGER,
    total_orders INTEGER DEFAULT 0,
    total_revenue DECIMAL(15, 2) DEFAULT 0,
    avg_order_value DECIMAL(15, 2),
    pending_orders INTEGER DEFAULT 0,
    completed_orders INTEGER DEFAULT 0,
    cancelled_orders INTEGER DEFAULT 0,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(metric_date, metric_hour, branch_id)
);
```

### Table: order_items_analytics
```sql
CREATE TABLE order_items_analytics (
    id SERIAL PRIMARY KEY,
    order_id INTEGER NOT NULL,
    product_id INTEGER NOT NULL,
    product_name VARCHAR(255),
    color VARCHAR(50),
    quantity INTEGER,
    unit_price DECIMAL(15, 2),
    total_price DECIMAL(15, 2),
    order_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ğŸ”§ Flink Jobs Dá»± kiáº¿n

### Job 1: OrderEventProcessor
**Má»¥c Ä‘Ã­ch**: Xá»­ lÃ½ táº¥t cáº£ order events vÃ  lÆ°u vÃ o database

**Input**: Kafka topics (order-created, order-updated, order-cancelled)
**Processing**:
- Deserialize JSON events
- Validate data
- Enrich vá»›i metadata
- Transform to database model

**Output**: 
- PostgreSQL: orders_raw, orders_processed

### Job 2: OrderStatusTracker
**Má»¥c Ä‘Ã­ch**: Theo dÃµi lá»‹ch sá»­ thay Ä‘á»•i tráº¡ng thÃ¡i

**Input**: Kafka topic (order-events) - filter event_type = "order.status_changed"
**Processing**:
- Track status changes
- Calculate processing time between statuses
- Detect anomalies (e.g., stuck orders)

**Output**: 
- PostgreSQL: order_status_history

### Job 3: RealTimeMetricsAggregator
**Má»¥c Ä‘Ã­ch**: TÃ­nh toÃ¡n metrics real-time

**Input**: All order events
**Processing**:
- Window aggregation (tumbling window per hour)
- Calculate:
  - Total orders
  - Total revenue
  - Average order value
  - Orders by status
  - Orders by branch

**Output**: 
- PostgreSQL: order_metrics

### Job 4: ProductAnalytics (Optional)
**Má»¥c Ä‘Ã­ch**: PhÃ¢n tÃ­ch sáº£n pháº©m Ä‘Æ°á»£c order

**Input**: order-created events
**Processing**:
- Extract order items
- Count product popularity
- Calculate revenue by product

**Output**: 
- PostgreSQL: order_items_analytics

## ğŸ“ Cáº¥u trÃºc Project

```
lensart_eyewear_backend/
â”‚
â”œâ”€â”€ app/                                    # Laravel Application
â”‚   â”œâ”€â”€ Http/
â”‚   â”‚   â””â”€â”€ Controllers/
â”‚   â”‚       â””â”€â”€ KafkaEventController.php    # Kafka event API
â”‚   â”œâ”€â”€ Services/
â”‚   â”‚   â””â”€â”€ KafkaService.php                # Kafka producer service
â”‚   â”œâ”€â”€ Events/
â”‚   â”‚   â””â”€â”€ OrderEvent.php
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ kafka.php                           # Laravel Kafka config
â”‚
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ kafka.api.php                       # Kafka API routes
â”‚
â”œâ”€â”€ data_pipeline/                          # ğŸ¯ Data Processing Pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ docker/                             # Docker Infrastructure
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml              # All services definition
â”‚   â”‚   â”œâ”€â”€ .env.docker                     # Docker environment variables
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”‚   â””â”€â”€ kafka-topics.sh             # Script táº¡o Kafka topics
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â”‚   â””â”€â”€ init.sql                    # Database schema initialization
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ flink/
â”‚   â”‚       â””â”€â”€ flink-conf.yaml             # Flink configuration (optional)
â”‚   â”‚
â”‚   â”œâ”€â”€ flink-jobs/                         # Flink Jobs Source Code
â”‚   â”‚   â”œâ”€â”€ pom.xml                         # Maven configuration
â”‚   â”‚   â”œâ”€â”€ README.md                       # Flink jobs documentation
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â””â”€â”€ main/
â”‚   â”‚           â”œâ”€â”€ java/
â”‚   â”‚           â”‚   â””â”€â”€ com/lensart/pipeline/
â”‚   â”‚           â”‚       â”‚
â”‚   â”‚           â”‚       â”œâ”€â”€ jobs/
â”‚   â”‚           â”‚       â”‚   â”œâ”€â”€ OrderEventProcessor.java
â”‚   â”‚           â”‚       â”‚   â”œâ”€â”€ OrderStatusTracker.java
â”‚   â”‚           â”‚       â”‚   â””â”€â”€ RealTimeMetricsAggregator.java
â”‚   â”‚           â”‚       â”‚
â”‚   â”‚           â”‚       â”œâ”€â”€ models/
â”‚   â”‚           â”‚       â”‚   â”œâ”€â”€ OrderEvent.java
â”‚   â”‚           â”‚       â”‚   â”œâ”€â”€ OrderMetrics.java
â”‚   â”‚           â”‚       â”‚   â””â”€â”€ OrderStatusHistory.java
â”‚   â”‚           â”‚       â”‚
â”‚   â”‚           â”‚       â”œâ”€â”€ serializers/
â”‚   â”‚           â”‚       â”‚   â””â”€â”€ OrderEventDeserializer.java
â”‚   â”‚           â”‚       â”‚
â”‚   â”‚           â”‚       â”œâ”€â”€ sinks/
â”‚   â”‚           â”‚       â”‚   â””â”€â”€ PostgresSink.java
â”‚   â”‚           â”‚       â”‚
â”‚   â”‚           â”‚       â””â”€â”€ utils/
â”‚   â”‚           â”‚           â”œâ”€â”€ KafkaConfig.java
â”‚   â”‚           â”‚           â””â”€â”€ DatabaseConfig.java
â”‚   â”‚           â”‚
â”‚   â”‚           â””â”€â”€ resources/
â”‚   â”‚               â”œâ”€â”€ application.properties
â”‚   â”‚               â””â”€â”€ log4j2.properties
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/                            # Helper Scripts
â”‚   â”‚   â”œâ”€â”€ start-all.sh                    # Start all Docker services
â”‚   â”‚   â”œâ”€â”€ stop-all.sh                     # Stop all services
â”‚   â”‚   â”œâ”€â”€ restart-all.sh                  # Restart services
â”‚   â”‚   â”œâ”€â”€ deploy-jobs.sh                  # Deploy Flink jobs
â”‚   â”‚   â”œâ”€â”€ reset-data.sh                   # Reset databases and topics
â”‚   â”‚   â””â”€â”€ test-flow.sh                    # Test end-to-end flow
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                              # Integration Tests
â”‚   â”‚   â”œâ”€â”€ test-kafka-connection.sh
â”‚   â”‚   â”œâ”€â”€ test-flink-jobs.sh
â”‚   â”‚   â””â”€â”€ generate-test-events.py
â”‚   â”‚
â”‚   â”œâ”€â”€ docs/                               # Documentation
â”‚   â”‚   â”œâ”€â”€ SETUP.md                        # Setup guide
â”‚   â”‚   â”œâ”€â”€ ARCHITECTURE.md                 # Architecture details
â”‚   â”‚   â”œâ”€â”€ JOBS.md                         # Flink jobs documentation
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md              # Common issues & solutions
â”‚   â”‚
â”‚   â””â”€â”€ README.md                           # Data pipeline overview
â”‚
â”œâ”€â”€ KAFKA_SETUP.md                          # Laravel Kafka integration
â”œâ”€â”€ KAFKA_FLINK_LOCAL_SETUP.md              # This file
â””â”€â”€ ...
```

### ğŸ“‚ Giáº£i thÃ­ch Folder Structure

**`data_pipeline/`** - Táº¥t cáº£ code vÃ  configs liÃªn quan Ä‘áº¿n data processing
- **`docker/`** - Docker Compose vÃ  configs cho Kafka, Flink, PostgreSQL
- **`flink-jobs/`** - Source code cá»§a cÃ¡c Flink jobs (Java/Maven)
- **`scripts/`** - Helper scripts Ä‘á»ƒ quáº£n lÃ½ pipeline
- **`tests/`** - Integration tests vÃ  test utilities
- **`docs/`** - Chi tiáº¿t documentation cho data pipeline

**Lá»£i Ã­ch:**
- âœ… Clear separation: Laravel app vs Data pipeline
- âœ… Easy to maintain vÃ  scale
- âœ… Independent development teams
- âœ… CÃ³ thá»ƒ reuse data_pipeline cho projects khÃ¡c
- âœ… Professional structure

## ğŸš€ CÃ¡c bÆ°á»›c cháº¡y (Dá»± kiáº¿n)

### BÆ°á»›c 1: Setup Docker Environment

```bash
cd lensart_eyewear_backend/data_pipeline

# Start all services using helper script
./scripts/start-all.sh

# Hoáº·c start manual:
docker-compose -f docker/docker-compose.yml up -d

# Kiá»ƒm tra services Ä‘ang cháº¡y
docker-compose -f docker/docker-compose.yml ps
```

**Expected output:**
```
NAME                 STATUS    PORTS
zookeeper            Up        2181
kafka                Up        9092
kafka-ui             Up        8080
flink-jobmanager     Up        8081
flink-taskmanager    Up        
postgres             Up        5432
pgadmin              Up        5050
```

### BÆ°á»›c 2: Khá»Ÿi táº¡o Kafka Topics

```bash
# Create all topics (REQUIRED - topics not auto-created)
docker exec kafka /usr/bin/kafka-topics --create \
  --topic order-created \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1 \
  --if-not-exists

docker exec kafka /usr/bin/kafka-topics --create \
  --topic order-updated \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1 \
  --if-not-exists

docker exec kafka /usr/bin/kafka-topics --create \
  --topic order-cancelled \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1 \
  --if-not-exists

docker exec kafka /usr/bin/kafka-topics --create \
  --topic order-events \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1 \
  --if-not-exists

docker exec kafka /usr/bin/kafka-topics --create \
  --topic order-events-dlq \
  --bootstrap-server localhost:9092 \
  --partitions 1 \
  --replication-factor 1 \
  --if-not-exists

# OR create all at once:
for topic in order-created order-updated order-cancelled order-events order-events-dlq; do
  docker exec kafka /usr/bin/kafka-topics --create \
    --topic $topic \
    --bootstrap-server localhost:9092 \
    --partitions 3 \
    --replication-factor 1 \
    --if-not-exists
done

# Verify topics Ä‘Ã£ Ä‘Æ°á»£c táº¡o:
docker exec kafka /usr/bin/kafka-topics --list --bootstrap-server localhost:9092
```

### BÆ°á»›c 3: Khá»Ÿi táº¡o Database Schema

```bash
# PostgreSQL sáº½ tá»± Ä‘á»™ng cháº¡y init.sql khi start láº§n Ä‘áº§u
# Verify database vÃ  tables:
docker exec -it postgres psql -U postgres -d lensart_events \
  -c "\dt"

# Hoáº·c re-initialize manual:
docker exec -it postgres psql -U postgres -d lensart_events \
  -f /docker-entrypoint-initdb.d/init.sql
```

### BÆ°á»›c 4: Build vÃ  Deploy Flink Jobs

```bash
# Build Flink jobs
cd data_pipeline/flink-jobs
mvn clean package

# Deploy táº¥t cáº£ jobs báº±ng script
cd ..
./scripts/deploy-jobs.sh

# Hoáº·c deploy manual tá»«ng job:
docker exec -it flink-jobmanager flink run \
  /opt/flink/jobs/OrderEventProcessor.jar

docker exec -it flink-jobmanager flink run \
  /opt/flink/jobs/OrderStatusTracker.jar

docker exec -it flink-jobmanager flink run \
  /opt/flink/jobs/RealTimeMetricsAggregator.jar

# Verify jobs Ä‘ang cháº¡y:
docker exec -it flink-jobmanager flink list
```

### BÆ°á»›c 5: Cáº¥u hÃ¬nh Laravel

```bash
# Quay vá» root project
cd ../..

# Update .env vá»›i local Kafka config
echo "KAFKA_BROKERS=localhost:9092" >> .env

# Clear cache
php artisan config:clear
php artisan cache:clear
```

### BÆ°á»›c 6: Test End-to-End Flow

```bash
# Terminal 1: Start Laravel API
php artisan serve

# Terminal 2: Test data pipeline
cd data_pipeline
./scripts/test-flow.sh

# Hoáº·c test manual:

# 1. Táº¡o order má»›i qua Laravel API (náº¿u cáº§n)
curl -X POST http://localhost:8000/api/orders/create \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "branch_id": 1,
    "address": "123 Test Street",
    "order_details": [
      {
        "product_id": 1,
        "color": "Black",
        "quantity": 1,
        "total_price": 500000
      }
    ]
  }'

# 2. Gá»­i order created event to Kafka
curl -X POST http://localhost:8000/api/kafka/events/order-created \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"order_id": 1}'

# 3. Check Kafka UI
open http://localhost:8080

# 4. Check Flink Dashboard
open http://localhost:8081

# 5. Check PostgreSQL data
docker exec -it postgres psql -U postgres -d lensart_events \
  -c "SELECT * FROM orders_raw ORDER BY id DESC LIMIT 5;"

# 6. Check processed data
docker exec -it postgres psql -U postgres -d lensart_events \
  -c "SELECT * FROM orders_processed ORDER BY id DESC LIMIT 5;"

# 7. Check metrics
docker exec -it postgres psql -U postgres -d lensart_events \
  -c "SELECT * FROM order_metrics ORDER BY updated_at DESC LIMIT 5;"
```

## ğŸ¨ Web Interfaces

| Service | URL | Purpose |
|---------|-----|---------|
| Kafka UI | http://localhost:8080 | Monitor Kafka topics, messages |
| Flink Dashboard | http://localhost:8081 | Monitor Flink jobs, metrics |
| PgAdmin | http://localhost:5050 | Manage PostgreSQL database |
| Laravel API | http://localhost:8000 | LensArt API endpoints |

## ğŸ“ Flow Code sáº½ implement

### Phase 1: Docker Infrastructure Setup
**Location:** `data_pipeline/docker/`

1. **docker-compose.yml**
   - Zookeeper service
   - Kafka broker vá»›i Kafka UI
   - Flink Job Manager & Task Manager
   - PostgreSQL vá»›i PgAdmin
   - Network configuration
   - Volume mounts

2. **kafka/kafka-topics.sh**
   - Script táº¡o topics: order-created, order-updated, order-cancelled, order-events
   - Auto-execute khi Kafka container start

3. **postgres/init.sql**
   - Create database `lensart_events`
   - Create 5 tables: orders_raw, orders_processed, order_status_history, order_metrics, order_items_analytics
   - Create indexes cho performance
   - Insert sample data (optional)

4. **.env.docker**
   - Environment variables cho cÃ¡c services
   - Kafka configs, PostgreSQL credentials, Flink settings

### Phase 2: Flink Jobs Development
**Location:** `data_pipeline/flink-jobs/`

1. **Setup Maven Project (pom.xml)**
   ```xml
   Dependencies:
   - Apache Flink 1.18.0
   - Flink Kafka Connector 3.0.0
   - PostgreSQL JDBC Driver 42.6.0
   - Jackson for JSON
   - Log4j2
   ```

2. **Job 1: OrderEventProcessor**
   - Source: Kafka topics (order-created, order-updated, order-cancelled)
   - Process: Deserialize, validate, enrich data
   - Sink: PostgreSQL (orders_raw, orders_processed)
   - Error handling & retry logic

3. **Job 2: OrderStatusTracker**
   - Source: Kafka (order-events)
   - Filter: event_type = "order.status_changed"
   - State: Track previous status per order
   - Process: Calculate processing time
   - Sink: PostgreSQL (order_status_history)

4. **Job 3: RealTimeMetricsAggregator**
   - Source: All order events
   - Window: Tumbling window (1 hour)
   - Aggregate: Count, sum, average by branch
   - Sink: PostgreSQL (order_metrics)
   - Update strategy: Upsert

5. **Supporting Classes**
   - Models: OrderEvent, OrderMetrics, OrderStatusHistory
   - Serializers: OrderEventDeserializer
   - Sinks: Custom PostgresSink with connection pooling
   - Utils: KafkaConfig, DatabaseConfig

### Phase 3: Helper Scripts
**Location:** `data_pipeline/scripts/`

1. **start-all.sh**
   ```bash
   - Start Docker Compose
   - Wait for services to be healthy
   - Verify Kafka topics created
   - Verify database initialized
   ```

2. **stop-all.sh**
   ```bash
   - Stop all Flink jobs gracefully
   - Stop Docker services
   - Clean up (optional)
   ```

3. **restart-all.sh**
   ```bash
   - Stop all services
   - Start all services
   - Redeploy Flink jobs
   ```

4. **deploy-jobs.sh**
   ```bash
   - Build Flink jobs (mvn package)
   - Copy JARs to Flink container
   - Submit jobs to Flink cluster
   - Verify jobs running
   ```

5. **reset-data.sh**
   ```bash
   - Delete Kafka topics
   - Truncate PostgreSQL tables
   - Recreate topics
   - Reseed data (optional)
   ```

6. **test-flow.sh**
   ```bash
   - Generate test events
   - Send to Laravel API
   - Verify data in Kafka
   - Verify data in PostgreSQL
   - Show success/failure report
   ```

### Phase 4: Integration Testing
**Location:** `data_pipeline/tests/`

1. **test-kafka-connection.sh**
   - Test Kafka broker connectivity
   - List topics
   - Produce/consume test messages

2. **test-flink-jobs.sh**
   - Check all jobs running
   - Verify job health
   - Check for exceptions in logs

3. **generate-test-events.py**
   - Python script to generate realistic order events
   - Send via Laravel API or directly to Kafka
   - Configurable event count and rate

### Phase 5: Documentation
**Location:** `data_pipeline/docs/`

1. **SETUP.md**
   - Prerequisites
   - Step-by-step setup guide
   - Environment configuration
   - Troubleshooting

2. **ARCHITECTURE.md**
   - System architecture diagram
   - Data flow explanation
   - Component details
   - Design decisions

3. **JOBS.md**
   - Each Flink job description
   - Input/output specifications
   - Configuration options
   - Performance tuning

4. **TROUBLESHOOTING.md**
   - Common issues and solutions
   - Debug commands
   - Log locations
   - Contact information

### Phase 6: Main README
**Location:** `data_pipeline/README.md`

- Overview of data pipeline
- Quick start guide
- Link to detailed docs
- Architecture diagram
- Team contacts

## ğŸ” Monitoring & Debugging

### Check Kafka Messages
```bash
# Console consumer Ä‘á»ƒ xem messages
docker exec kafka /usr/bin/kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic order-created \
  --from-beginning

# Press Ctrl+C to stop
```

### Check Flink Logs
```bash
# Job Manager logs
docker logs flink-jobmanager

# Task Manager logs
docker logs flink-taskmanager
```

### Check PostgreSQL Data
```bash
# Connect to PostgreSQL
docker exec -it postgres psql -U postgres -d lensart_events

# Query data
SELECT COUNT(*) FROM orders_raw;
SELECT * FROM order_metrics ORDER BY updated_at DESC LIMIT 10;
```

## âš ï¸ LÆ°u Ã½ quan trá»ng

1. **Resource Requirements**:
   - RAM: Minimum 8GB (recommended 16GB)
   - CPU: Minimum 4 cores
   - Disk: 10GB free space

2. **Port Conflicts**: Äáº£m báº£o cÃ¡c ports sau chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng:
   - 2181 (Zookeeper)
   - 9092 (Kafka)
   - 8080 (Kafka UI)
   - 8081 (Flink)
   - 5432 (PostgreSQL)
   - 5050 (PgAdmin)

3. **Development Flow**:
   - Develop Flink jobs locally
   - Build JAR files
   - Deploy to Docker Flink cluster
   - Test vá»›i real events tá»« Laravel

4. **Data Persistence**:
   - Kafka data: Docker volume `kafka-data`
   - PostgreSQL data: Docker volume `postgres-data`
   - Data sáº½ persist khi restart containers

## ğŸ“¦ Dependencies

### Laravel (ÄÃ£ cÃ i)
- nmred/kafka-php: ^0.1.6

### Flink Jobs (Sáº½ cÃ i)
- Apache Flink: 1.18.0
- Flink Kafka Connector: 3.0.0
- PostgreSQL JDBC Driver: 42.6.0

### Docker Images
- confluentinc/cp-kafka:7.5.0
- confluentinc/cp-zookeeper:7.5.0
- provectuslabs/kafka-ui:latest
- flink:1.18.0-scala_2.12
- postgres:16-alpine
- dpage/pgadmin4:latest

## ğŸ¯ Success Criteria

Setup thÃ nh cÃ´ng khi:
- âœ… All Docker containers running
- âœ… Kafka topics created
- âœ… PostgreSQL database initialized
- âœ… Flink jobs deployed and running
- âœ… Laravel API cÃ³ thá»ƒ gá»­i events
- âœ… Events flow tá»« Laravel â†’ Kafka â†’ Flink â†’ PostgreSQL
- âœ… Data xuáº¥t hiá»‡n trong PostgreSQL tables
- âœ… Web UIs accessible

## ğŸ“ Troubleshooting

### Kafka khÃ´ng connect Ä‘Æ°á»£c
```bash
# Check Kafka logs
docker logs kafka

# Test connection
telnet localhost 9092
```

### Flink job failed
```bash
# Check job status
docker exec flink-jobmanager flink list

# Check logs
docker logs flink-jobmanager --tail 100
```

### PostgreSQL connection refused
```bash
# Check if PostgreSQL is running
docker exec postgres pg_isready -U postgres

# Check logs
docker logs postgres
```

---

## ğŸ‘¨â€ğŸ’» Development Workflow

### LÃ m viá»‡c vá»›i Laravel (Backend Team)
```bash
# Root project
cd lensart_eyewear_backend

# Develop Laravel features
php artisan serve

# Test Kafka integration
curl -X POST http://localhost:8000/api/kafka/events/order-created \
  -H "Authorization: Bearer TOKEN" \
  -d '{"order_id": 1}'
```

### LÃ m viá»‡c vá»›i Data Pipeline (Data Team)
```bash
# Data pipeline directory
cd lensart_eyewear_backend/data_pipeline

# Start infrastructure
./scripts/start-all.sh

# Develop Flink jobs
cd flink-jobs
# Edit Java code...
mvn clean package

# Deploy updated jobs
cd ..
./scripts/deploy-jobs.sh

# Test pipeline
./scripts/test-flow.sh

# Check logs
docker logs flink-jobmanager --tail 50

# Stop when done
./scripts/stop-all.sh
```

### Team Collaboration
```
Backend Team (app/)          Data Team (data_pipeline/)
      â”‚                               â”‚
      â”œâ”€ API Development             â”œâ”€ Docker Setup
      â”œâ”€ KafkaService                â”œâ”€ Flink Jobs
      â”œâ”€ Event Models                â”œâ”€ Database Schema
      â””â”€ API Testing                 â””â”€ Pipeline Testing
                â”‚                     â”‚
                â””â”€â”€â”€â”€ Integration â”€â”€â”€â”€â”˜
                    (Kafka Events)
```

## ğŸ“… Timeline Thá»±c hiá»‡n

| Phase | Task | Estimated Time |
|-------|------|----------------|
| 1 | Docker Infrastructure (docker-compose.yml, init scripts) | 45 mins |
| 2 | Database Schema & Init SQL | 30 mins |
| 3 | Helper Scripts (start, stop, deploy, test) | 45 mins |
| 4 | Flink Job 1 - OrderEventProcessor | 2.5 hours |
| 5 | Flink Job 2 - OrderStatusTracker | 1.5 hours |
| 6 | Flink Job 3 - RealTimeMetricsAggregator | 2 hours |
| 7 | Integration Testing & Bug Fixes | 1.5 hours |
| 8 | Documentation (SETUP, ARCHITECTURE, JOBS, TROUBLESHOOTING) | 1 hour |
| **Total** | | **~10 hours** |

## ğŸ¯ Next Steps

### Immediate (Báº¯t Ä‘áº§u ngay)
1. âœ… Táº¡o folder structure `data_pipeline/`
2. âœ… Setup Docker Compose vá»›i Kafka + Flink + PostgreSQL
3. âœ… Táº¡o database schema (init.sql)
4. âœ… Táº¡o helper scripts (start-all.sh, stop-all.sh)
5. âœ… Test infrastructure locally

### Short-term (1-2 tuáº§n)
1. â³ Develop Flink Job 1 (OrderEventProcessor)
2. â³ Test event flow: Laravel â†’ Kafka â†’ Flink â†’ PostgreSQL
3. â³ Develop Flink Job 2 & 3
4. â³ Complete integration testing

### Long-term (Sau khi demo thÃ nh cÃ´ng)
1. ğŸ”® Add monitoring (Prometheus + Grafana)
2. ğŸ”® Implement CI/CD pipeline
3. ğŸ”® Deploy to Azure (AKS + Event Hubs)
4. ğŸ”® Scale testing vá»›i high volume
5. ğŸ”® Add more analytics features

---

## âœ¨ Káº¿t luáº­n

Vá»›i cáº¥u trÃºc folder `data_pipeline/` má»›i:

**âœ… Advantages:**
- Clear separation of concerns
- Independent development & deployment
- Easy to maintain and scale
- Professional project structure
- Reusable for other projects

**ğŸ“‚ Structure Summary:**
```
lensart_eyewear_backend/
â”œâ”€â”€ app/                     # Laravel (Backend Team)
â”œâ”€â”€ data_pipeline/           # Data Processing (Data Team)
â”‚   â”œâ”€â”€ docker/              # Infrastructure as Code
â”‚   â”œâ”€â”€ flink-jobs/          # Stream Processing Logic
â”‚   â”œâ”€â”€ scripts/             # Automation Scripts
â”‚   â”œâ”€â”€ tests/               # Integration Tests
â”‚   â””â”€â”€ docs/                # Detailed Documentation
â””â”€â”€ ...
```

**ğŸš€ Sáºµn sÃ ng Ä‘á»ƒ implement!**

Báº¡n cÃ³ muá»‘n tÃ´i báº¯t Ä‘áº§u vá»›i Phase 1 (Docker Infrastructure Setup) khÃ´ng?

